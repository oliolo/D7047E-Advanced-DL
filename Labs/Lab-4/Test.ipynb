{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/input.txt\", \"r\")\n",
    "text = f.read()\n",
    "data_size = len(text)\n",
    "\n",
    "dictionary = []\n",
    "for i in text:\n",
    "    if i not in dictionary:\n",
    "        dictionary.append(i)\n",
    "#print(dictionary)\n",
    "\n",
    "chunk_len = 100\n",
    "print()\n",
    "dataset = []\n",
    "k = 0\n",
    "for char in text:\n",
    "    ind = [dictionary.index(char)]\n",
    "    dataset.append(ind)\n",
    "\n",
    "for i in range(math.floor(len(text)/chunk_len)):\n",
    "    word = []\n",
    "    for j in range(k,k+chunk_len):\n",
    "        word.append(text[j])\n",
    "    k += chunk_len\n",
    "    dataset.append(word)\n",
    "dataset = text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'z', 'e', 'n', ':', '\\n', 'B', 'f', 'o', 'w', 'p', 'c', 'd', 'a', 'y', 'u', 'h', ',', 'm', 'k', '.', 'A', 'l', 'S', 'Y', 'v', '?', 'R', 'M', 'W', \"'\", 'L', 'I', 'N', 'g', ';', 'b', '!', 'O', 'j', 'V', '-', 'T', 'H', 'E', 'U', 'D', 'P', 'q', 'x', 'J', 'G', 'K', 'Q', '&', 'Z', 'X', '3', '$']\n",
      "['First', 'Citizen:', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.']\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)\n",
    "print(dataset[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 200   # size of hidden state\n",
    "seq_len = 100       # length of LSTM sequence\n",
    "num_layers = 3      # num of layers in LSTM layer stack\n",
    "learning_rate = 0.001          # learning rate\n",
    "weight_decay = 1e-5            #weight decay\n",
    "epochs = 20        # max number of epochs\n",
    "drop_prob = 0.5\n",
    "vocab_size = len(dictionary)  #length pf dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, drop_prob):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, input_size)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        #self.drop_out = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        embedds = self.embedding(x)\n",
    "        out, hidden_state = self.lstm(embedds, hidden_state)\n",
    "        #out = self.drop_out(out)\n",
    "        out = self.fc(out)\n",
    "        return out, (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charRnn = CharRNN(vocab_size, vocab_size, hidden_size, num_layers, drop_prob)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(charRnn.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cfa8e1fd62d7eb8beb134597a3e6a4ed7d63db87a242cb567554c600c91ca9b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('anaconda_backup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
