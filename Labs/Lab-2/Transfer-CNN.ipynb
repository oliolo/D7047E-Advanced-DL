{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "trainset_MNIST_big = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainset_MNIST = torch.utils.data.Subset(trainset_MNIST_big,list(range(10000)))\n",
    "trainloader_MNIST = torch.utils.data.DataLoader(trainset_MNIST, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "val_set_MNIST = torch.utils.data.Subset(trainset_MNIST_big,list(range(10000, 20000)))\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(val_set_MNIST, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "                                          \n",
    "testset_MNIST = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader_MNIST = torch.utils.data.DataLoader(testset_MNIST, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = models.resnet18(pretrained= False)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_model = model\n",
    "    best_loss = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_correct = 0\n",
    "        tr_total = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        for batch_nr, (data, labels) in enumerate(train_loader):\n",
    "            \n",
    "            print(\"Epoch: \",epoch,\"Batch: \",batch_nr)\n",
    "            # calculate prediction according to our model\n",
    "            \n",
    "            prediction = model.forward(data)\n",
    "            # Calculate the loss of the prediction by comparing to the expected output\n",
    "            loss = criterion(prediction, labels)\n",
    "            \n",
    "            # Backpropagate the loss through the network to find the gradients of all parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters along their gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Clear stored gradient values\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # calculate accuracy\n",
    "            for i in range(len(data)):    \n",
    "                guess = torch.argmax(prediction[i])\n",
    "                if(guess.item() == labels[i]):\n",
    "                    tr_correct+=1\n",
    "                tr_total +=1\n",
    "\n",
    "        for batch_nr, (data, labels) in enumerate(val_loader):\n",
    "            \n",
    "            prediction = model.forward(data)\n",
    "            \n",
    "            # Calculate the loss of the prediction by comparing to the expected output\n",
    "            loss = criterion(prediction, labels)\n",
    "\n",
    "            if(loss < best_loss):\n",
    "                best_loss = loss\n",
    "                best_model = model\n",
    "\n",
    "            # calculate accuracy\n",
    "            for i in range(len(data)):    \n",
    "                guess = torch.argmax(prediction[i])\n",
    "                if(guess.item() == labels[i]):\n",
    "                    val_correct+=1\n",
    "                val_total +=1\n",
    "\n",
    "    # primt accuracy\n",
    "    tr_accuracy = tr_correct/tr_total\n",
    "    val_accuracy = val_correct/val_total\n",
    "    print(f'Training accuracy:   {str(100*tr_accuracy)[:4]}%.')\n",
    "    print(f'Validation accuracy: {str(100*val_accuracy)[:4]}%.')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    for batch_nr, (data, labels) in enumerate(test_loader):\n",
    "        prediction = model.forward(data)\n",
    "        \n",
    "        # calculate accuracy\n",
    "        for i in range(len(data)):    \n",
    "            guess = torch.argmax(prediction[i])\n",
    "            if(guess.item() == labels[i]):\n",
    "                val_correct+=1\n",
    "            val_total +=1\n",
    "\n",
    "    # primt accuracy\n",
    "    val_accuracy = val_correct/val_total\n",
    "    print(f'Test accuracy: {str(100*val_accuracy)[:4]}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Batch:  0\n",
      "Epoch:  0 Batch:  1\n",
      "Epoch:  0 Batch:  2\n",
      "Epoch:  0 Batch:  3\n",
      "Epoch:  0 Batch:  4\n",
      "Epoch:  0 Batch:  5\n",
      "Epoch:  0 Batch:  6\n",
      "Epoch:  0 Batch:  7\n",
      "Epoch:  0 Batch:  8\n",
      "Epoch:  0 Batch:  9\n",
      "Epoch:  0 Batch:  10\n",
      "Epoch:  0 Batch:  11\n",
      "Epoch:  0 Batch:  12\n",
      "Epoch:  0 Batch:  13\n",
      "Epoch:  0 Batch:  14\n",
      "Epoch:  0 Batch:  15\n",
      "Epoch:  0 Batch:  16\n",
      "Epoch:  0 Batch:  17\n",
      "Epoch:  0 Batch:  18\n",
      "Epoch:  0 Batch:  19\n",
      "Epoch:  1 Batch:  0\n",
      "Epoch:  1 Batch:  1\n",
      "Epoch:  1 Batch:  2\n",
      "Epoch:  1 Batch:  3\n",
      "Epoch:  1 Batch:  4\n",
      "Epoch:  1 Batch:  5\n",
      "Epoch:  1 Batch:  6\n",
      "Epoch:  1 Batch:  7\n",
      "Epoch:  1 Batch:  8\n",
      "Epoch:  1 Batch:  9\n",
      "Epoch:  1 Batch:  10\n",
      "Epoch:  1 Batch:  11\n",
      "Epoch:  1 Batch:  12\n",
      "Epoch:  1 Batch:  13\n",
      "Epoch:  1 Batch:  14\n",
      "Epoch:  1 Batch:  15\n",
      "Epoch:  1 Batch:  16\n",
      "Epoch:  1 Batch:  17\n",
      "Epoch:  1 Batch:  18\n",
      "Epoch:  1 Batch:  19\n",
      "Epoch:  2 Batch:  0\n",
      "Epoch:  2 Batch:  1\n",
      "Epoch:  2 Batch:  2\n",
      "Epoch:  2 Batch:  3\n",
      "Epoch:  2 Batch:  4\n",
      "Epoch:  2 Batch:  5\n",
      "Epoch:  2 Batch:  6\n",
      "Epoch:  2 Batch:  7\n",
      "Epoch:  2 Batch:  8\n",
      "Epoch:  2 Batch:  9\n",
      "Epoch:  2 Batch:  10\n",
      "Epoch:  2 Batch:  11\n",
      "Epoch:  2 Batch:  12\n",
      "Epoch:  2 Batch:  13\n",
      "Epoch:  2 Batch:  14\n",
      "Epoch:  2 Batch:  15\n",
      "Epoch:  2 Batch:  16\n",
      "Epoch:  2 Batch:  17\n",
      "Epoch:  2 Batch:  18\n",
      "Epoch:  2 Batch:  19\n",
      "Epoch:  3 Batch:  0\n",
      "Epoch:  3 Batch:  1\n",
      "Epoch:  3 Batch:  2\n",
      "Epoch:  3 Batch:  3\n",
      "Epoch:  3 Batch:  4\n",
      "Epoch:  3 Batch:  5\n",
      "Epoch:  3 Batch:  6\n",
      "Epoch:  3 Batch:  7\n",
      "Epoch:  3 Batch:  8\n",
      "Epoch:  3 Batch:  9\n",
      "Epoch:  3 Batch:  10\n",
      "Epoch:  3 Batch:  11\n",
      "Epoch:  3 Batch:  12\n",
      "Epoch:  3 Batch:  13\n",
      "Epoch:  3 Batch:  14\n",
      "Epoch:  3 Batch:  15\n",
      "Epoch:  3 Batch:  16\n",
      "Epoch:  3 Batch:  17\n",
      "Epoch:  3 Batch:  18\n",
      "Epoch:  3 Batch:  19\n",
      "Epoch:  4 Batch:  0\n",
      "Epoch:  4 Batch:  1\n",
      "Epoch:  4 Batch:  2\n",
      "Epoch:  4 Batch:  3\n",
      "Epoch:  4 Batch:  4\n",
      "Epoch:  4 Batch:  5\n",
      "Epoch:  4 Batch:  6\n",
      "Epoch:  4 Batch:  7\n",
      "Epoch:  4 Batch:  8\n",
      "Epoch:  4 Batch:  9\n",
      "Epoch:  4 Batch:  10\n",
      "Epoch:  4 Batch:  11\n",
      "Epoch:  4 Batch:  12\n",
      "Epoch:  4 Batch:  13\n",
      "Epoch:  4 Batch:  14\n",
      "Epoch:  4 Batch:  15\n",
      "Epoch:  4 Batch:  16\n",
      "Epoch:  4 Batch:  17\n",
      "Epoch:  4 Batch:  18\n",
      "Epoch:  4 Batch:  19\n",
      "Epoch:  5 Batch:  0\n",
      "Epoch:  5 Batch:  1\n",
      "Epoch:  5 Batch:  2\n",
      "Epoch:  5 Batch:  3\n",
      "Epoch:  5 Batch:  4\n",
      "Epoch:  5 Batch:  5\n",
      "Epoch:  5 Batch:  6\n",
      "Epoch:  5 Batch:  7\n",
      "Epoch:  5 Batch:  8\n",
      "Epoch:  5 Batch:  9\n",
      "Epoch:  5 Batch:  10\n",
      "Epoch:  5 Batch:  11\n",
      "Epoch:  5 Batch:  12\n",
      "Epoch:  5 Batch:  13\n",
      "Epoch:  5 Batch:  14\n",
      "Epoch:  5 Batch:  15\n",
      "Epoch:  5 Batch:  16\n",
      "Epoch:  5 Batch:  17\n",
      "Epoch:  5 Batch:  18\n",
      "Epoch:  5 Batch:  19\n",
      "Epoch:  6 Batch:  0\n",
      "Epoch:  6 Batch:  1\n",
      "Epoch:  6 Batch:  2\n",
      "Epoch:  6 Batch:  3\n",
      "Epoch:  6 Batch:  4\n",
      "Epoch:  6 Batch:  5\n",
      "Epoch:  6 Batch:  6\n",
      "Epoch:  6 Batch:  7\n",
      "Epoch:  6 Batch:  8\n",
      "Epoch:  6 Batch:  9\n",
      "Epoch:  6 Batch:  10\n",
      "Epoch:  6 Batch:  11\n",
      "Epoch:  6 Batch:  12\n",
      "Epoch:  6 Batch:  13\n",
      "Epoch:  6 Batch:  14\n",
      "Epoch:  6 Batch:  15\n",
      "Epoch:  6 Batch:  16\n",
      "Epoch:  6 Batch:  17\n",
      "Epoch:  6 Batch:  18\n",
      "Epoch:  6 Batch:  19\n",
      "Epoch:  7 Batch:  0\n",
      "Epoch:  7 Batch:  1\n",
      "Epoch:  7 Batch:  2\n",
      "Epoch:  7 Batch:  3\n",
      "Epoch:  7 Batch:  4\n",
      "Epoch:  7 Batch:  5\n",
      "Epoch:  7 Batch:  6\n",
      "Epoch:  7 Batch:  7\n",
      "Epoch:  7 Batch:  8\n",
      "Epoch:  7 Batch:  9\n",
      "Epoch:  7 Batch:  10\n",
      "Epoch:  7 Batch:  11\n",
      "Epoch:  7 Batch:  12\n",
      "Epoch:  7 Batch:  13\n",
      "Epoch:  7 Batch:  14\n",
      "Epoch:  7 Batch:  15\n",
      "Epoch:  7 Batch:  16\n",
      "Epoch:  7 Batch:  17\n",
      "Epoch:  7 Batch:  18\n",
      "Epoch:  7 Batch:  19\n",
      "Epoch:  8 Batch:  0\n",
      "Epoch:  8 Batch:  1\n",
      "Epoch:  8 Batch:  2\n",
      "Epoch:  8 Batch:  3\n",
      "Epoch:  8 Batch:  4\n",
      "Epoch:  8 Batch:  5\n",
      "Epoch:  8 Batch:  6\n",
      "Epoch:  8 Batch:  7\n",
      "Epoch:  8 Batch:  8\n",
      "Epoch:  8 Batch:  9\n",
      "Epoch:  8 Batch:  10\n",
      "Epoch:  8 Batch:  11\n",
      "Epoch:  8 Batch:  12\n",
      "Epoch:  8 Batch:  13\n",
      "Epoch:  8 Batch:  14\n",
      "Epoch:  8 Batch:  15\n",
      "Epoch:  8 Batch:  16\n",
      "Epoch:  8 Batch:  17\n",
      "Epoch:  8 Batch:  18\n",
      "Epoch:  8 Batch:  19\n",
      "Epoch:  9 Batch:  0\n",
      "Epoch:  9 Batch:  1\n",
      "Epoch:  9 Batch:  2\n",
      "Epoch:  9 Batch:  3\n",
      "Epoch:  9 Batch:  4\n",
      "Epoch:  9 Batch:  5\n",
      "Epoch:  9 Batch:  6\n",
      "Epoch:  9 Batch:  7\n",
      "Epoch:  9 Batch:  8\n",
      "Epoch:  9 Batch:  9\n",
      "Epoch:  9 Batch:  10\n",
      "Epoch:  9 Batch:  11\n",
      "Epoch:  9 Batch:  12\n",
      "Epoch:  9 Batch:  13\n",
      "Epoch:  9 Batch:  14\n",
      "Epoch:  9 Batch:  15\n",
      "Epoch:  9 Batch:  16\n",
      "Epoch:  9 Batch:  17\n",
      "Epoch:  9 Batch:  18\n",
      "Epoch:  9 Batch:  19\n",
      "Training accuracy:   100.%.\n",
      "Validation accuracy: 96.2%.\n",
      "Test accuracy: 96.2%.\n"
     ]
    }
   ],
   "source": [
    "Trained_model = train(model, criterion, optimizer, trainloader_MNIST, val_loader_MNIST, num_epochs=10)\n",
    "test(Trained_model, testloader_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Trained_model, './CNN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data\\train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182041600it [00:25, 7267607.79it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data\\test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64275456it [00:10, 6095374.53it/s]                               \n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "all_svhn_train_data = torchvision.datasets.SVHN(root='./data', split= 'train', download=True, transform=transform)\n",
    "all_svhn_test_data = torchvision.datasets.SVHN(root='./data', split= 'test', download=True, transform=transform)\n",
    "\n",
    "svhn_train_set = torch.utils.data.Subset(all_svhn_train_data,list(range(10000)))\n",
    "svhn_val_set  = torch.utils.data.Subset(all_svhn_train_data,list(range(10000, 20000)))\n",
    "svhn_test_set  = torch.utils.data.Subset(all_svhn_test_data,list(range(10000)))\n",
    "\n",
    "svhn_train_loader = torch.utils.data.DataLoader(svhn_train_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "svhn_val_loader = torch.utils.data.DataLoader(svhn_val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "svhn_test_loader = torch.utils.data.DataLoader(svhn_test_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 10.3%.\n"
     ]
    }
   ],
   "source": [
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "test(Trained_model, svhn_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mnistCNN = torch.load('./CNN_model')\n",
    "\n",
    "trained_resnet_fe = Trained_model \n",
    "\n",
    "# FREEZE all old params\n",
    "for param in trained_resnet_fe.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# new layer\n",
    "num_ftrs = trained_resnet_fe.fc.in_features\n",
    "trained_resnet_fe.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# find the paramaters we want to update during training\n",
    "params_to_update = []\n",
    "for param in trained_resnet_fe.parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "# Define our loss function\n",
    "criterion_fe = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer_fe = torch.optim.Adam(params_to_update, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Batch:  0\n",
      "Epoch:  0 Batch:  1\n",
      "Epoch:  0 Batch:  2\n",
      "Epoch:  0 Batch:  3\n",
      "Epoch:  0 Batch:  4\n",
      "Epoch:  0 Batch:  5\n",
      "Epoch:  0 Batch:  6\n",
      "Epoch:  0 Batch:  7\n",
      "Epoch:  0 Batch:  8\n",
      "Epoch:  0 Batch:  9\n",
      "Epoch:  0 Batch:  10\n",
      "Epoch:  0 Batch:  11\n",
      "Epoch:  0 Batch:  12\n",
      "Epoch:  0 Batch:  13\n",
      "Epoch:  0 Batch:  14\n",
      "Epoch:  0 Batch:  15\n",
      "Epoch:  0 Batch:  16\n",
      "Epoch:  0 Batch:  17\n",
      "Epoch:  0 Batch:  18\n",
      "Epoch:  0 Batch:  19\n",
      "Epoch:  1 Batch:  0\n",
      "Epoch:  1 Batch:  1\n",
      "Epoch:  1 Batch:  2\n",
      "Epoch:  1 Batch:  3\n",
      "Epoch:  1 Batch:  4\n",
      "Epoch:  1 Batch:  5\n",
      "Epoch:  1 Batch:  6\n",
      "Epoch:  1 Batch:  7\n",
      "Epoch:  1 Batch:  8\n",
      "Epoch:  1 Batch:  9\n",
      "Epoch:  1 Batch:  10\n",
      "Epoch:  1 Batch:  11\n",
      "Epoch:  1 Batch:  12\n",
      "Epoch:  1 Batch:  13\n",
      "Epoch:  1 Batch:  14\n",
      "Epoch:  1 Batch:  15\n",
      "Epoch:  1 Batch:  16\n",
      "Epoch:  1 Batch:  17\n",
      "Epoch:  1 Batch:  18\n",
      "Epoch:  1 Batch:  19\n",
      "Epoch:  2 Batch:  0\n",
      "Epoch:  2 Batch:  1\n",
      "Epoch:  2 Batch:  2\n",
      "Epoch:  2 Batch:  3\n",
      "Epoch:  2 Batch:  4\n",
      "Epoch:  2 Batch:  5\n",
      "Epoch:  2 Batch:  6\n",
      "Epoch:  2 Batch:  7\n",
      "Epoch:  2 Batch:  8\n",
      "Epoch:  2 Batch:  9\n",
      "Epoch:  2 Batch:  10\n",
      "Epoch:  2 Batch:  11\n",
      "Epoch:  2 Batch:  12\n",
      "Epoch:  2 Batch:  13\n",
      "Epoch:  2 Batch:  14\n",
      "Epoch:  2 Batch:  15\n",
      "Epoch:  2 Batch:  16\n",
      "Epoch:  2 Batch:  17\n",
      "Epoch:  2 Batch:  18\n",
      "Epoch:  2 Batch:  19\n",
      "Epoch:  3 Batch:  0\n",
      "Epoch:  3 Batch:  1\n",
      "Epoch:  3 Batch:  2\n",
      "Epoch:  3 Batch:  3\n",
      "Epoch:  3 Batch:  4\n",
      "Epoch:  3 Batch:  5\n",
      "Epoch:  3 Batch:  6\n",
      "Epoch:  3 Batch:  7\n",
      "Epoch:  3 Batch:  8\n",
      "Epoch:  3 Batch:  9\n",
      "Epoch:  3 Batch:  10\n",
      "Epoch:  3 Batch:  11\n",
      "Epoch:  3 Batch:  12\n",
      "Epoch:  3 Batch:  13\n",
      "Epoch:  3 Batch:  14\n",
      "Epoch:  3 Batch:  15\n",
      "Epoch:  3 Batch:  16\n",
      "Epoch:  3 Batch:  17\n",
      "Epoch:  3 Batch:  18\n",
      "Epoch:  3 Batch:  19\n",
      "Epoch:  4 Batch:  0\n",
      "Epoch:  4 Batch:  1\n",
      "Epoch:  4 Batch:  2\n",
      "Epoch:  4 Batch:  3\n",
      "Epoch:  4 Batch:  4\n",
      "Epoch:  4 Batch:  5\n",
      "Epoch:  4 Batch:  6\n",
      "Epoch:  4 Batch:  7\n",
      "Epoch:  4 Batch:  8\n",
      "Epoch:  4 Batch:  9\n",
      "Epoch:  4 Batch:  10\n",
      "Epoch:  4 Batch:  11\n",
      "Epoch:  4 Batch:  12\n",
      "Epoch:  4 Batch:  13\n",
      "Epoch:  4 Batch:  14\n",
      "Epoch:  4 Batch:  15\n",
      "Epoch:  4 Batch:  16\n",
      "Epoch:  4 Batch:  17\n",
      "Epoch:  4 Batch:  18\n",
      "Epoch:  4 Batch:  19\n",
      "Epoch:  5 Batch:  0\n",
      "Epoch:  5 Batch:  1\n",
      "Epoch:  5 Batch:  2\n",
      "Epoch:  5 Batch:  3\n",
      "Epoch:  5 Batch:  4\n",
      "Epoch:  5 Batch:  5\n",
      "Epoch:  5 Batch:  6\n",
      "Epoch:  5 Batch:  7\n",
      "Epoch:  5 Batch:  8\n",
      "Epoch:  5 Batch:  9\n",
      "Epoch:  5 Batch:  10\n",
      "Epoch:  5 Batch:  11\n",
      "Epoch:  5 Batch:  12\n",
      "Epoch:  5 Batch:  13\n",
      "Epoch:  5 Batch:  14\n",
      "Epoch:  5 Batch:  15\n",
      "Epoch:  5 Batch:  16\n",
      "Epoch:  5 Batch:  17\n",
      "Epoch:  5 Batch:  18\n",
      "Epoch:  5 Batch:  19\n",
      "Epoch:  6 Batch:  0\n",
      "Epoch:  6 Batch:  1\n",
      "Epoch:  6 Batch:  2\n",
      "Epoch:  6 Batch:  3\n",
      "Epoch:  6 Batch:  4\n",
      "Epoch:  6 Batch:  5\n",
      "Epoch:  6 Batch:  6\n",
      "Epoch:  6 Batch:  7\n",
      "Epoch:  6 Batch:  8\n",
      "Epoch:  6 Batch:  9\n",
      "Epoch:  6 Batch:  10\n",
      "Epoch:  6 Batch:  11\n",
      "Epoch:  6 Batch:  12\n",
      "Epoch:  6 Batch:  13\n",
      "Epoch:  6 Batch:  14\n",
      "Epoch:  6 Batch:  15\n",
      "Epoch:  6 Batch:  16\n",
      "Epoch:  6 Batch:  17\n",
      "Epoch:  6 Batch:  18\n",
      "Epoch:  6 Batch:  19\n",
      "Epoch:  7 Batch:  0\n",
      "Epoch:  7 Batch:  1\n",
      "Epoch:  7 Batch:  2\n",
      "Epoch:  7 Batch:  3\n",
      "Epoch:  7 Batch:  4\n",
      "Epoch:  7 Batch:  5\n",
      "Epoch:  7 Batch:  6\n",
      "Epoch:  7 Batch:  7\n",
      "Epoch:  7 Batch:  8\n",
      "Epoch:  7 Batch:  9\n",
      "Epoch:  7 Batch:  10\n",
      "Epoch:  7 Batch:  11\n",
      "Epoch:  7 Batch:  12\n",
      "Epoch:  7 Batch:  13\n",
      "Epoch:  7 Batch:  14\n",
      "Epoch:  7 Batch:  15\n",
      "Epoch:  7 Batch:  16\n",
      "Epoch:  7 Batch:  17\n",
      "Epoch:  7 Batch:  18\n",
      "Epoch:  7 Batch:  19\n",
      "Epoch:  8 Batch:  0\n",
      "Epoch:  8 Batch:  1\n",
      "Epoch:  8 Batch:  2\n",
      "Epoch:  8 Batch:  3\n",
      "Epoch:  8 Batch:  4\n",
      "Epoch:  8 Batch:  5\n",
      "Epoch:  8 Batch:  6\n",
      "Epoch:  8 Batch:  7\n",
      "Epoch:  8 Batch:  8\n",
      "Epoch:  8 Batch:  9\n",
      "Epoch:  8 Batch:  10\n",
      "Epoch:  8 Batch:  11\n",
      "Epoch:  8 Batch:  12\n",
      "Epoch:  8 Batch:  13\n",
      "Epoch:  8 Batch:  14\n",
      "Epoch:  8 Batch:  15\n",
      "Epoch:  8 Batch:  16\n",
      "Epoch:  8 Batch:  17\n",
      "Epoch:  8 Batch:  18\n",
      "Epoch:  8 Batch:  19\n",
      "Epoch:  9 Batch:  0\n",
      "Epoch:  9 Batch:  1\n",
      "Epoch:  9 Batch:  2\n",
      "Epoch:  9 Batch:  3\n",
      "Epoch:  9 Batch:  4\n",
      "Epoch:  9 Batch:  5\n",
      "Epoch:  9 Batch:  6\n",
      "Epoch:  9 Batch:  7\n",
      "Epoch:  9 Batch:  8\n",
      "Epoch:  9 Batch:  9\n",
      "Epoch:  9 Batch:  10\n",
      "Epoch:  9 Batch:  11\n",
      "Epoch:  9 Batch:  12\n",
      "Epoch:  9 Batch:  13\n",
      "Epoch:  9 Batch:  14\n",
      "Epoch:  9 Batch:  15\n",
      "Epoch:  9 Batch:  16\n",
      "Epoch:  9 Batch:  17\n",
      "Epoch:  9 Batch:  18\n",
      "Epoch:  9 Batch:  19\n",
      "Training accuracy:   32.4%.\n",
      "Validation accuracy: 25.8%.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the training step\n",
    "train(trained_resnet_fe, criterion_fe, optimizer_fe, svhn_train_loader, svhn_val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction of MNIST-trained Resnet on SVHN dataset:\n",
      "Test accuracy: 26.0%.\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Extraction of MNIST-trained Resnet on SVHN dataset:\")\n",
    "test(trained_resnet_fe, svhn_test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78d776e26a726845a150a952c27a68eddb22d28f3f64186fed14956ccdb93b0a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('D7047E_ADL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
